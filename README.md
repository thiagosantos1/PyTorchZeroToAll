# Pytorch Notes  --> https://www.youtube.com/watch?v=ogZi5oIo4fI&list=PLlMkM4tgfjnJ3I-dbhO9JTw7gNty6o_2m&index=12

Every operation is saved as a graph.

Thus, we can have the use of special methods, for instance:

l = loss(x,y)
l.backward() # comes back and calculate gradiente descente for all weights

w.data = w.data - 0.01 * w.grad.data - Store the gradiente at w.grad
 0.01 is the learning rate



***** IMPORTANT ******
Gotta zero the gradiente, to 



* Chain Rule --> "easer" with pytorch, since it's a graph 
We can compute the gradient at each node


* Python Variables

w = Variable(torch.Tensor([1.0]), requires_grad=True)

Once Pytorch sees the "Variable" or the "Tensor", it already creates a graph around that Variable.

That is really good, since for each node it already calculate the gradiente for us

# Logic


1) First, we want to do forward pass ( This is where we conect the parts available to create the arquiteture of our model)

2) Second, we want to calculate the Gradiente from the Loss

3) Then, update based on gradient


# Pytorch steps

1) Design your model using class with Variables --> This is the most important part. This would go in your definition
   It can be a very complicated LSTM, or CNN or others.

2) Construct loss and optimizer ( from PyTorch API)

3) Training cycle --> Forward, Backward, Update



# How to construct the model ?

We can use multiple Linear combinations to create our model. Example 07:
The idea is that each linear can be multiplied agains the weights
We can also add layers , as seen below. In that case, we can add squash functions between layers.

self.l1 = nn.Linear(8, 6) # 8 features as input and 6 as ouput

self.l2 = nn.Linear(6, 4)

self.l3 = nn.Linear(4, 1)

self.sigmoid = nn.Sigmoid() # Not a really good activation function for Backpropagation

In this case,  Why these numbers ? 

these numbers are arbitrary. Same as a Hidden layers. This is your Neural Network
Thus, you must choose those numbers. 
However, The first one always has to be the same as your input dimension 
and the last must be same as your desired output dimension


# DataLoader - Very Important

It's important to have automatic data loader and fetch. We do not want to feed all at once for our model.
Sometimes we want to do in baches.

train_loader = DataLoader(dataset=dataset,batch_size=32,shuffle=True,num_workers=2)

# Softmax Classifier - Used for multiple classification problems

It can get an output and create a nice probability for each digit.

Usually, the Y is a hot encoding vector, with 1 where is true.

To calculate the loss, we use the entropy. Since it's a hot encoding, it can easily calculate 



# CNN - Convolutional Neural Net - Weights are shared


Convolution: Look only at small portions of image, by using filters. We look at the whole picture, but small parts by time.
This is good to find the "best" most important features of the image towards the target.

Basically, with get a part of the image, let's say 3x3, and we multiply by our filter/weights and create a single value witch represents
that 3x3 part of the image. This can then abstract the important features


Pooling/ Subsampling : Want to reduce the amount of information generated by the conv. Usually, we created a new squared of information
and we then just choose the max of that square. This can reduce the number of features. Another idea is to the get the average


* Arquiteture:

Input layer --> Conv2 --> Pooling --> Conv--> pooling --> ........ Fully connected / flat --> Softmax


# RNN 

Each state/node has the information from previous nodes. This is very usefull for NLP





# Pytorch Notes  --> https://www.youtube.com/watch?v=ogZi5oIo4fI&list=PLlMkM4tgfjnJ3I-dbhO9JTw7gNty6o_2m&index=12

Every operation is saved as a graph.

Thus, we can have the use of special methods, for instance:

l = loss(x,y)
l.backward() # comes back and calculate gradiente descente for all weights

w.data = w.data - 0.01 * w.grad.data - Store the gradiente at w.grad
 0.01 is the learning rate



***** IMPORTANT ******
Gotta zero the gradiente, to 



* Chain Rule --> "easer" with pytorch, since it's a graph 
We can compute the gradient at each node


* Python Variables

w = Variable(torch.Tensor([1.0]), requires_grad=True)

Once Pytorch sees the "Variable" or the "Tensor", it already creates a graph around that Variable.

That is really good, since for each node it already calculate the gradiente for us

# Logic


1) First, we want to do forward pass ( This is where we conect the parts available to create the arquiteture of our model)

2) Second, we want to calculate the Gradiente from the Loss

3) Then, update based on gradient


# Pytorch steps

1) Design your model using class with Variables --> This is the most important part. This would go in your definition
   It can be a very complicated LSTM, or CNN or others.

2) Construct loss and optimizer ( from PyTorch API)

3) Training cycle --> Forward, Backward, Update



# How to construct the model ?

We can use multiple Linear combinations to create our model. Example 07:
The idea is that each linear can be multiplied agains the weights
We can also add layers , as seen below. In that case, we can add squash functions between layers.

self.l1 = nn.Linear(8, 6) # 8 features as input and 6 as ouput

self.l2 = nn.Linear(6, 4)

self.l3 = nn.Linear(4, 1)

self.sigmoid = nn.Sigmoid() # Not a really good activation function for Backpropagation

In this case,  Why these numbers ? 

these numbers are arbitrary. Same as a Hidden layers. This is your Neural Network
Thus, you must choose those numbers. 
However, The first one always has to be the same as your input dimension 
and the last must be same as your desired output dimension


# DataLoader - Very Important

It's important to have automatic data loader and fetch. We do not want to feed all at once for our model.
Sometimes we want to do in baches.

train_loader = DataLoader(dataset=dataset,batch_size=32,shuffle=True,num_workers=2)

# Softmax Classifier - Used for multiple classification problems

It can get an output and create a nice probability for each digit.

Usually, the Y is a hot encoding vector, with 1 where is true.

To calculate the loss, we use the entropy. Since it's a hot encoding, it can easily calculate 



# CNN - Convolutional Neural Net - Weights are shared


Convolution: Look only at small portions of image, by using filters. We look at the whole picture, but small parts by time.
This is good to find the "best" most important features of the image towards the target.

Basically, with get a part of the image, let's say 3x3, and we multiply by our filter/weights and create a single value witch represents
that 3x3 part of the image. This can then abstract the important features


Pooling/ Subsampling : Want to reduce the amount of information generated by the conv. Usually, we created a new squared of information
and we then just choose the max of that square. This can reduce the number of features. Another idea is to the get the average


* Arquiteture:

Input layer --> Conv2 --> Pooling --> Conv--> pooling --> ........ Fully connected / flat --> Softmax


# RNN 

Each state/node has the information from previous nodes. This is very usefull for NLP


* Logic:

At state X, we get the info from previous hidden state X-1(Which also brings info from previous states)
Then we concatenate the info from the cell itself, do a Matrix multiplication together with a tanh and produce an output.
This output is the output of the cell which also goes as input for next cell state

In Pytorch we have:
1) nn.RNN(input_size=x, hidden_size=y, batch_first=True)
2) nn.GRU(input_size=x, hidden_size=y, batch_first=True)
3) nn.LSTM(input_size=x, hidden_size=y, batch_first=True)

These numbers means: at each state, we gonna have an input/vector of size x(Let's say x=4 words) and and output of 
size y(Let's say y=2 words)

Basically this hidden is passed to next cells


* Hello Word example:

Let's say we want to give hello as input, how can we do it? 

We can use One hot encoding for example. In this case, for letter h = [1,0,0,0], input size = 4

For the output/hidden state, we can choose. Let's say 2. This means the RNN will produce a vector of size 2, with some values between 0 and 1(probability)


* But feeding letter by letter may no be efficiente. We want to use sequence of letters. 

This is called seq_len. 

Let's say seq_len = 5. Thus input_size = (1,5,4) for HELLO as one hot

* We can also provide multiple words at once. Example: hello word

In that case, we have a batch size which correspond for each letter.
our input size would be = (2,5,4)

* How does it work then:

Simple. When training, as we said, a state X produces an ouput y. that ouput is then feed to state X+1, which then tries to produce the next output.

Example: State 1 has h as input and then produces i as output. State 2 then has i as input and produce h as output. 
State 3 has h as input, but since it has information from previous states, it produce e as output, instead of i. and so on.


In the case of 5 different letters only, we gonna use a vector of size 5 for hotencoding. The output then, is also a one hotencoding of size 5.

* Loss ?

We use Cross Entropy as before. Since we want to see which information/letters are more relevant/give more information for the right output. Since is a multi classification with probability.


* Embeddings - Very useful

Instead of using one hotencoding, we shall use embeddings, which is way better. How to insert that in out code?

Pytorch already have that for us --> self.embedding = nn.Embedding(input_size, embedding_size)

# Pad Sequences

When your data have different lengths for each input string. If you using batch, that become a problem.

The solution is to create padding for each vector, based on the length of the longest one
